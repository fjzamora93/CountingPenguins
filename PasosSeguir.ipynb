{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar YOLO y sus dependencias\n",
    "!pip install tensorflow keras opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Convertir las coordenadas del CSV en bounding boxes\n",
    "\n",
    "Las coordenadas que tienes (Top_Left_X, Top_Left_Y, Bottom_Right_X, Bottom_Right_Y) definen rectángulos donde se encuentran los pingüinos en la imagen. Estas coordenadas serán las \"bounding boxes\" o cajas delimitadoras que tu modelo de detección de objetos debe aprender a predecir.\n",
    "\n",
    "Tendrás que asegurarte de que estas coordenadas estén en el formato correcto (si ya están en píxeles, estás listo). Si estuvieran en un sistema de referencia espacial, puedes transformarlas a píxeles para que sean compatibles con las imágenes TIFF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Generar etiquetas en formato YOLO o Faster R-CNN\n",
    "\n",
    "Dependiendo del modelo que elijas, deberás convertir las coordenadas en el formato específico que requieren los frameworks de entrenamiento. Para YOLO, las etiquetas deberían estar en un formato como [class_id, center_x, center_y, width, height] normalizado entre 0 y 1. \n",
    "\n",
    "[class_id, x_center, y_center, width, height]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from rasterio.transform import from_origin\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-59.231611</td>\n",
       "      <td>-62.301249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-59.231621</td>\n",
       "      <td>-62.301249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-59.231525</td>\n",
       "      <td>-62.301251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-59.231545</td>\n",
       "      <td>-62.301249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-59.231539</td>\n",
       "      <td>-62.301256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id          X          Y\n",
       "0   0 -59.231611 -62.301249\n",
       "1   0 -59.231621 -62.301249\n",
       "2   0 -59.231525 -62.301251\n",
       "3   0 -59.231545 -62.301249\n",
       "4   0 -59.231539 -62.301256"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datos de entrada\n",
    "CSV_COORDS = 'tiles_coords/coords_62.csv'\n",
    "df = pd.read_csv(CSV_COORDS)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TILE = 62\n",
    "\n",
    "if os.path.exists('/content/drive/My Drive/doctorado_albert/conteo_pinguinos/'):\n",
    "    path  = '/content/drive/My Drive/doctorado_albert/conteo_pinguinos/'\n",
    "else:\n",
    "    path = 'G:\\\\.shortcut-targets-by-id\\\\1pYgV5EIk4-LapLNhlCwpQaDAzuqNffXG\\\\doctorado_albert\\\\conteo_pinguinos'\n",
    "\n",
    "image_name = f\"recortes/recorte_{NUM_TILE}.tif\"\n",
    "tiff_file = os.path.join(path, image_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 2: Descargar el modelo YOLO preentrenado\n",
    "Para este ejemplo, vamos a usar YOLOv5 de la biblioteca de Ultralytics, que es una de las implementaciones más populares y fáciles de usar. Puedes clonar su repositorio y seguir las instrucciones.\n",
    "\n",
    "# Clonar el repositorio de YOLOv5\n",
    "git clone https://github.com/ultralytics/yolov5.git\n",
    "cd yolov5\n",
    "\n",
    "# Instalar dependencias\n",
    "pip install -r requirements.txt\n",
    "\n",
    "2. Preparar los Datos\n",
    "Los datos de entrenamiento deben estar en el formato adecuado para YOLO. Asegúrate de que tus coordenadas normalizadas estén guardadas en un archivo .txt para cada imagen.\n",
    "\n",
    "Además, la estructura de directorios debería ser esta:\n",
    "\n",
    "yolov5/\n",
    "├── data/\n",
    "│   ├── penguin_dataset.yaml   # Archivo de configuración de tu dataset\n",
    "│   ├── images/\n",
    "│   │   ├── train/              # Imágenes de entrenamiento\n",
    "│   │   ├── val/                # Etiquetas de validación\n",
    "│   └── labels/\n",
    "│       ├── train/              # Etiquetas de entrenamiento\n",
    "│       ├── val/                # Etiquetas de validación\n",
    "├── yolov5/\n",
    "└── train.py  \n",
    "\n",
    "Cada archivo de etiqueta debe ser un archivo de texto .txt que tenga el mismo nombre que su imagen correspondiente. Por ejemplo, si tienes una imagen image1.jpg, el archivo de etiqueta debe llamarse image1.txt. y debe incluir dentro la siguiente información:\n",
    "\n",
    "<class_id> <x_center> <y_center> <width> <height>\n",
    "\n",
    "Estas etiquetas deben estar NORMALIZADAS, es decir, con valores entre 0 y 1.\n",
    "\n",
    "\n",
    "Al final, deberías tener una estructura de directorios similar a esta:\n",
    "\n",
    "data/\n",
    "├── images/\n",
    "│   ├── train/\n",
    "│   │   ├── penguin1.jpg\n",
    "│   │   ├── penguin2.jpg\n",
    "│   │   └── penguin3.jpg\n",
    "│   └── val/\n",
    "│       ├── penguin_val1.jpg\n",
    "│       └── penguin_val2.jpg\n",
    "└── labels/\n",
    "    ├── train/\n",
    "    │   ├── penguin1.txt\n",
    "    │   ├── penguin2.txt\n",
    "    │   └── penguin3.txt\n",
    "    └── val/\n",
    "        ├── penguin_val1.txt\n",
    "        └── penguin_val2.txt\n",
    "\n",
    "Si tienes 100 imágenes de entrenamiento en la carpeta de IMAGES, tendrás 100 documentos de txt con su id, x, y, ancho y alto en la carpeta de LABELS.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 3: Cargar el modelo preentrenado\n",
    "Una vez que tengas la biblioteca YOLOv5 instalada, puedes cargar un modelo preentrenado fácilmente. Aquí hay un ejemplo de cómo hacerlo en un script de Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Cargar el modelo preentrenado (puedes usar 'yolov5s', 'yolov5m', 'yolov5l', 'yolov5x')\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "# Establecer el modelo en modo de evaluación\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 4: Realizar detección en imágenes\n",
    "Una vez que tengas el modelo, puedes usarlo para hacer detección en imágenes. Aquí te muestro cómo hacerlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Cargar una imagen (asegúrate de tener la ruta correcta a la imagen)\n",
    "image_path = 'ruta/a/tu/imagen.jpg'\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Realizar la detección\n",
    "results = model(image)\n",
    "\n",
    "# Mostrar los resultados\n",
    "results.show()  # Muestra la imagen con bounding boxes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 5: Procesar los resultados\n",
    "Los resultados de YOLO contienen información sobre las detecciones, incluyendo las coordenadas de las cajas delimitadoras, las clases y las puntuaciones de confianza. Aquí tienes un ejemplo de cómo acceder a esta información:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las detecciones\n",
    "detections = results.xyxy[0]  # formato [x1, y1, x2, y2, conf, class]\n",
    "\n",
    "for detection in detections:\n",
    "    x1, y1, x2, y2, conf, cls = detection.numpy()\n",
    "    print(f'Clase: {model.names[int(cls)]}, Confianza: {conf:.2f}, Coordenadas: [{x1:.0f}, {y1:.0f}, {x2:.0f}, {y2:.0f}]')\n",
    "\n",
    "# Si quieres dibujar las detecciones en la imagen\n",
    "for detection in detections:\n",
    "    x1, y1, x2, y2, conf, cls = detection.numpy()\n",
    "    cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)  # Dibuja la caja\n",
    "    cv2.putText(image, f'{model.names[int(cls)]} {conf:.2f}', (int(x1), int(y1)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "# Mostrar la imagen con detecciones\n",
    "cv2.imshow('Detección', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 6: Ajustar el modelo para tu caso específico (opcional)\n",
    "Si quieres mejorar la precisión del modelo para detectar pingüinos específicamente, puedes hacer fine-tuning del modelo preentrenado usando tu propio conjunto de datos. Esto implicaría:\n",
    "\n",
    "Preparar un conjunto de datos con tus imágenes y anotaciones.\n",
    "Entrenar el modelo usando el conjunto de datos específico. Para esto, puedes seguir las instrucciones de entrenamiento en el repositorio de YOLOv5.\n",
    "Conclusión\n",
    "Estos pasos te permitirán empezar a trabajar con un modelo YOLO preentrenado utilizando Keras y PyTorch. Si decides hacer fine-tuning o necesitas más ayuda sobre cómo ajustar el modelo, no dudes en preguntar. ¡Buena suerte con tu proyecto!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
