{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testando Yolov8\n",
    "\n",
    "Dentro de este notebook procederemos a testar un conjunto de imagens com o modelo Yolov8.\n",
    "\n",
    "Para ello, realizaremos lo siguiente:\n",
    "- Cortaremos un ortomosáico -igual que hicimos para el conjunto de training.\n",
    "- Prepararemos las etiquetas de dicho ortomosaico.\n",
    "- Realizaremos la predicción con el modelo Yolov8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.yolo_fun as yolo_fun\n",
    "import utils.img_fun as img_fun\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  \n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from rasterio.errors import RasterioIOError\n",
    "import numpy as np\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#orthomosiac_coords = os.path.join('coords', 'yolo_coords.csv')\n",
    "orthomosiac_coords = os.path.join('test', 'test_coords.csv')\n",
    "\n",
    "coords_dir_sin_normalizar = os.path.join('test', 'labels_sin_normalizar')\n",
    "coords_dir_normalized = os.path.join('test', 'labels_normalized')\n",
    "subrecortes_dir = os.path.join('test', 'img')\n",
    "\n",
    "\n",
    "os.makedirs(coords_dir_sin_normalizar, exist_ok=True)\n",
    "os.makedirs(coords_dir_normalized, exist_ok=True)\n",
    "os.makedirs(subrecortes_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "#path_doctorado = 'G:\\\\.shortcut-targets-by-id\\\\1pYgV5EIk4-LapLNhlCwpQaDAzuqNffXG\\\\doctorado_albert\\\\conteo_pinguinos\\\\recortes'\n",
    "\n",
    "path_doctorado = 'G:\\\\.shortcut-targets-by-id\\\\1pYgV5EIk4-LapLNhlCwpQaDAzuqNffXG\\\\doctorado_albert\\\\pinguiton\\\\ortho_dic_5000'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Recortando imagen imagen-15-8.tif...\n",
      "_________________________________________________________\n",
      "Metadata:\n",
      "---------\n",
      "driver: GTiff\n",
      "dtype: uint8\n",
      "nodata: None\n",
      "width: 5000\n",
      "height: 5000\n",
      "count: 4\n",
      "crs: EPSG:4326\n",
      "transform: | 0.00, 0.00,-59.21|\n",
      "| 0.00,-0.00,-62.31|\n",
      "| 0.00, 0.00, 1.00|\n",
      "blockxsize: 256\n",
      "blockysize: 256\n",
      "tiled: True\n",
      "compress: lzw\n",
      "interleave: pixel\n",
      "\n",
      "Coordenadas de las esquinas de la imagen:\n",
      "TOP LEFT: (-59.213980474998074, -62.307177528123795)\n",
      "BOTTOM RIGHT: (-59.212102454998075, -62.3080516081238)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "crop_tile_into_subrecortes() got an unexpected keyword argument 'rows'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m     min_x, max_y \u001b[38;5;241m=\u001b[39m img_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_left\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     26\u001b[0m     max_x, min_y \u001b[38;5;241m=\u001b[39m img_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbottom_right\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 28\u001b[0m     \u001b[43mimg_fun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrop_tile_into_subrecortes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtiff_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtiff_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msubrecortes_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoords_csv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43morthomosiac_coords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RasterioIOError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError al cargar la imagen con rasterio: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: crop_tile_into_subrecortes() got an unexpected keyword argument 'rows'"
     ]
    }
   ],
   "source": [
    "\n",
    "# PARTE 0: ITERAMOS SOBRE LAS IMÁGENES PARA RECORTARLAS\n",
    "\n",
    "contador = 0\n",
    "for img in os.listdir(path_doctorado):\n",
    "    contador += 1\n",
    "    if contador == 2: break\n",
    "    \n",
    "    try:\n",
    "        # PARTE 1: Cargar la imagen y recortarla en imágenes más pequeñas de aproximadamente 500x500 píxeles\n",
    "        print(f\"\\n\\nRecortando imagen {img}...\")\n",
    "        print('_________________________________________________________')\n",
    "\n",
    "        img_name = img.split('.')[0]\n",
    "        \n",
    "\n",
    "        tiff_file = os.path.join(path_doctorado, img_name + '.tif')\n",
    "\n",
    "\n",
    "        # Sacamos un diccionario con toda la información de la imagen\n",
    "        img_info = img_fun.get_img_info(tiff_file)\n",
    "        WIDTH = img_info[\"width\"]\n",
    "        HEIGHT = img_info[\"height\"]\n",
    "        TOP_LEFT = img_info[\"top_left\"]\n",
    "        BOTTOM_RIGHT = img_info[\"bottom_right\"]\n",
    "        min_x, max_y = img_info['top_left']\n",
    "        max_x, min_y = img_info['bottom_right']\n",
    "\n",
    "        img_fun.crop_tile_into_subrecortes(\n",
    "            tiff_file = tiff_file, \n",
    "            output_dir = subrecortes_dir, \n",
    "            coords_csv = orthomosiac_coords,\n",
    "            tile_size = 640,\n",
    "            overlap = 0\n",
    "        )\n",
    "        \n",
    "     \n",
    "    except RasterioIOError as e:\n",
    "        print(f\"Error al cargar la imagen con rasterio: {e}\")\n",
    "        continue\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        continue\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando archivos .txt: 100%|██████████| 44/44 [00:00<00:00, 108.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos .txt generados en coords\\labels_sin_normalizar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# PARTE 3: ASIGNACIÓN DE LABELS EN TXT A CADA SUBRECORTE\n",
    "\n",
    "yolo_fun.generar_txt_yolo(\n",
    "    subrecorte_dir = subrecortes_dir, \n",
    "    csv_file = orthomosiac_coords, \n",
    "    coords_dir = coords_dir_sin_normalizar\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Iteramos para normalizar las coordenadas de cara archivo.txt\n",
    "for file in os.listdir(coords_dir_sin_normalizar):\n",
    "    df_sin_normalizar = pd.read_csv(os.path.join(coords_dir_sin_normalizar, file), sep=' ', header=None)\n",
    "    name_subrecorte = os.path.splitext(file)[0]\n",
    "    subrecorte_file = os.path.join('cut_tiles', f\"{name_subrecorte}.tiff\")\n",
    "\n",
    "    print(f\"Normalizando archivo {file}...\")\n",
    "    coords_file = os.path.join(coords_dir_sin_normalizar, file)\n",
    "    output_file = os.path.join(coords_dir_normalized, file)\n",
    "    \n",
    "    yolo_fun.normalize_yolo_coords(\n",
    "        tiff_file = subrecorte_file,\n",
    "        coords_sin_normalizar = df_sin_normalizar, \n",
    "        output_file = output_file, \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se procesaron 1120 imágenes para entrenamiento y 281 imágenes para validación.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# PARTE 4: CLASIFICAR CONJUNTOS DE TRAIN Y VAL\n",
    "\n",
    "# Listar todas las imágenes y sus respectivos archivos de coordenadas\n",
    "images = [img for img in os.listdir(subrecortes_dir) if img.endswith('.tiff')]\n",
    "annotations = [os.path.join(coords_dir_normalized, f'{os.path.splitext(img)[0]}.txt') for img in images]\n",
    "\n",
    "# Filtrar solo las imágenes que tienen un archivo de coordenadas no vacío\n",
    "valid_images = []\n",
    "valid_annotations = []\n",
    "for img, txt_path in zip(images, annotations):\n",
    "    if os.path.exists(txt_path) and os.path.getsize(txt_path) > 0:\n",
    "        valid_images.append(img)\n",
    "        valid_annotations.append(txt_path)\n",
    "\n",
    "# Dividir las imágenes y etiquetas en conjuntos de entrenamiento (80%) y validación (20%)\n",
    "train_images, val_images, train_annotations, val_annotations = train_test_split(\n",
    "    valid_images, valid_annotations, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Crear directorios de salida si no existen\n",
    "os.makedirs('./datasets/penguin_dataset/images/train', exist_ok=True)\n",
    "os.makedirs('./datasets/penguin_dataset/images/val', exist_ok=True)\n",
    "os.makedirs('./datasets/penguin_dataset/labels/train', exist_ok=True)\n",
    "os.makedirs('./datasets/penguin_dataset/labels/val', exist_ok=True)\n",
    "\n",
    "# Copiar archivos al conjunto de entrenamiento\n",
    "for img, txt in zip(train_images, train_annotations):\n",
    "    shutil.copy(os.path.join(subrecortes_dir, img), './datasets/penguin_dataset/images/train')\n",
    "    shutil.copy(txt, './datasets/penguin_dataset/labels/train')\n",
    "\n",
    "# Copiar archivos al conjunto de validación\n",
    "for img, txt in zip(val_images, val_annotations):\n",
    "    shutil.copy(os.path.join(subrecortes_dir, img), './datasets/penguin_dataset/images/val')\n",
    "    shutil.copy(txt, './datasets/penguin_dataset/labels/val')\n",
    "\n",
    "print(f\"Se procesaron {len(train_images)} imágenes para entrenamiento y {len(val_images)} imágenes para validación.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
